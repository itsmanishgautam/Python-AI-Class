{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to job_data111.csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# User agent header\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "\n",
    "# List to store URLs\n",
    "urls = []\n",
    "\n",
    "# Iterate over pages 1 to 13\n",
    "for page_num in range(1, 14):\n",
    "    url = f'https://www.jobsnepal.com/jobs?page={page_num}'\n",
    "    urls.append(url)\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "job_titles = []\n",
    "companies = []\n",
    "locations = []\n",
    "categories = []\n",
    "\n",
    "# Iterate over each URL\n",
    "for url in urls:\n",
    "    # Send a request with the user agent header\n",
    "    request = urllib.request.Request(url, headers={'User-Agent': user_agent})\n",
    "    response = urllib.request.urlopen(request)\n",
    "    html_content = response.read().decode('utf-8')\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all job title elements\n",
    "    title_elements = soup.find_all('h2', class_='job-title')\n",
    "    for title_element in title_elements:\n",
    "        job_titles.append(title_element.text.strip())\n",
    "\n",
    "    # Find all company elements\n",
    "    company_elements = soup.find_all('div', class_='company-logo')\n",
    "    for company_element in company_elements:\n",
    "        companies.append(company_element.a.get('title'))\n",
    "\n",
    "    # Find all location elements\n",
    "    location_elements = soup.find_all('i', class_='icon-location4')\n",
    "    for location_element in location_elements:\n",
    "        locations.append(location_element.find_next_sibling('div').text.strip())\n",
    "\n",
    "    # Find all category elements\n",
    "    category_elements = soup.find_all('div', class_='icon-price-tags')\n",
    "    for category_element in category_elements:\n",
    "        categories.append(category_element.find_next('div').text.strip())\n",
    "\n",
    "# Create a list of tuples containing job data\n",
    "data = list(zip(job_titles, companies, locations, categories))\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file = 'job_data111.csv'\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Job Title', 'Company', 'Location', 'Category'])  # Write header row\n",
    "    writer.writerows(data)  # Write data rows\n",
    "\n",
    "print(f'Data exported to {csv_file}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
